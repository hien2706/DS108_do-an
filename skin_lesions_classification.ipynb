{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Aa36bMKLze3z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import keras.layers as kl\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from  matplotlib import pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "%matplotlib inline\n",
    "import shutil\n",
    "from sklearn.metrics import  precision_score, recall_score, accuracy_score,classification_report ,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lnzRzk7e44HL",
    "outputId": "6d1e2d2f-1669-42a6-f6a7-45a52b73ce41"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>dx</th>\n",
       "      <th>sex</th>\n",
       "      <th>lesion_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_4695565</td>\n",
       "      <td>75.0</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>akiec</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_2485486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_4407081</td>\n",
       "      <td>65.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_5173845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_6803155</td>\n",
       "      <td>70.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>male</td>\n",
       "      <td>IL_8876724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_6037786</td>\n",
       "      <td>55.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_1728878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_4263601</td>\n",
       "      <td>75.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>male</td>\n",
       "      <td>IL_6263448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  age_approx anatom_site_general     dx     sex   lesion_id\n",
       "0  ISIC_4695565        75.0     posterior torso  akiec  female  IL_2485486\n",
       "1  ISIC_4407081        65.0           head/neck  akiec  female  IL_5173845\n",
       "2  ISIC_6803155        70.0           head/neck  akiec    male  IL_8876724\n",
       "3  ISIC_6037786        55.0           head/neck  akiec  female  IL_1728878\n",
       "4  ISIC_4263601        75.0           head/neck  akiec    male  IL_6263448"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd = pd.read_csv('D:\\DS_2nd_year\\semester_2nd\\[DS108]\\Doan\\dataset_ds108\\isic_dermnet.csv')\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "_IFqPgUu5jPj",
    "outputId": "979e5d51-8319-435c-f4db-1981b452c849"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>dx</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lesion_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DERMNETakiec_0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DERMNETakiec_1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DERMNETakiec_10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DERMNETakiec_11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DERMNETakiec_12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 image_id  age_approx  anatom_site_general  dx  sex\n",
       "lesion_id                                                          \n",
       "DERMNETakiec_0          1           0                    0   1    0\n",
       "DERMNETakiec_1          1           0                    0   1    0\n",
       "DERMNETakiec_10         1           0                    0   1    0\n",
       "DERMNETakiec_11         1           0                    0   1    0\n",
       "DERMNETakiec_12         1           0                    0   1    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_count = data_pd.groupby('lesion_id').count()\n",
    "df_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "QjMQNZRI2xl7"
   },
   "outputs": [],
   "source": [
    "df_count = df_count[df_count['dx'] == 1]\n",
    "df_count.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NeVfs-Ly95gs"
   },
   "outputs": [],
   "source": [
    "def duplicates(x):\n",
    "    unique = set(df_count['lesion_id'])\n",
    "    if x in unique:\n",
    "        return 'no' \n",
    "    else:\n",
    "        return 'duplicates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "2WZZRSzO5v8t",
    "outputId": "5042fd49-03f1-48a6-ed7b-ac849f1fe0c9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>dx</th>\n",
       "      <th>sex</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_4695565</td>\n",
       "      <td>75.0</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>akiec</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_2485486</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_4407081</td>\n",
       "      <td>65.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_5173845</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_6803155</td>\n",
       "      <td>70.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>male</td>\n",
       "      <td>IL_8876724</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_6037786</td>\n",
       "      <td>55.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_1728878</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_4263601</td>\n",
       "      <td>75.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>male</td>\n",
       "      <td>IL_6263448</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  age_approx anatom_site_general     dx     sex   lesion_id  \\\n",
       "0  ISIC_4695565        75.0     posterior torso  akiec  female  IL_2485486   \n",
       "1  ISIC_4407081        65.0           head/neck  akiec  female  IL_5173845   \n",
       "2  ISIC_6803155        70.0           head/neck  akiec    male  IL_8876724   \n",
       "3  ISIC_6037786        55.0           head/neck  akiec  female  IL_1728878   \n",
       "4  ISIC_4263601        75.0           head/neck  akiec    male  IL_6263448   \n",
       "\n",
       "  is_duplicate  \n",
       "0           no  \n",
       "1           no  \n",
       "2           no  \n",
       "3           no  \n",
       "4           no  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd['is_duplicate'] = data_pd['lesion_id'].apply(duplicates)\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3BhGlAv0yAHu"
   },
   "outputs": [],
   "source": [
    "df_count = data_pd[data_pd['is_duplicate'] == 'no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_smallest_dimensions(folder_path):\n",
    "    min_width = float('inf')\n",
    "    min_height = float('inf')\n",
    "\n",
    "    # Loop through every file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".JPG\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with Image.open(file_path) as img:\n",
    "                width, height = img.size\n",
    "                if width < min_width:\n",
    "                    min_width = width\n",
    "                if height < min_height:\n",
    "                    min_height = height\n",
    "\n",
    "    return min_width, min_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211, 172)\n"
     ]
    }
   ],
   "source": [
    "print(find_smallest_dimensions('D:\\DS_2nd_year\\semester_2nd\\[DS108]\\Doan\\dataset_ds108\\images'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dir_4000 = os.path.join('dataset_ds108', 'my_dir_4000')\n",
    "os.mkdir(my_dir_4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hair removal \n",
    "\n",
    "def scale_remove_hair(input_path,output_path):\n",
    "    #Read image\n",
    "    image=cv2.imread(input_path,cv2.IMREAD_COLOR)\n",
    "    \n",
    "    image_size = 224\n",
    "    img = cv2.resize(image,(image_size,image_size))\n",
    "\n",
    "    #DULL RAZOR (REMOVE HAIR)\n",
    "\n",
    "    #Gray scale\n",
    "    grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY )\n",
    "    #Black hat filter\n",
    "    kernel = cv2.getStructuringElement(1,(9,9)) \n",
    "    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "    #Gaussian filter\n",
    "    bhg= cv2.GaussianBlur(blackhat,(3,3),cv2.BORDER_DEFAULT)\n",
    "    #Binary thresholding (MASK)\n",
    "    ret,mask = cv2.threshold(bhg,10,255,cv2.THRESH_BINARY)\n",
    "    #Replace pixels of the mask\n",
    "    dst = cv2.inpaint(img,mask,6,cv2.INPAINT_TELEA)   \n",
    "\n",
    "    cv2.imwrite(output_path,dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hair_removed_ = os.path.join(my_dir_4000, 'dataset_hair_removed_')\n",
    "os.makedirs(dataset_hair_removed_, exist_ok=True)\n",
    "for image_id in data_pd['image_id']:\n",
    "    image_path = os.path.join('D:\\DS_2nd_year\\semester_2nd\\[DS108]\\Doan\\dataset_ds108\\images', image_id + '.JPG')\n",
    "    new_image_path = os.path.join(dataset_hair_removed_, image_id + '.jpg')\n",
    "    scale_remove_hair(image_path,new_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Y3ndAO_Ex5fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\preprocess\\Lib\\site-packages\\sklearn\\utils\\validation.py:605: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\preprocess\\Lib\\site-packages\\sklearn\\utils\\validation.py:614: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "train, test_df = train_test_split(df_count, test_size=0.15, stratify=df_count['dx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "T7w2kYUdNkjX",
    "outputId": "5b6b9660-79f4-4fdb-ec79-4d5366f99a3e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>dx</th>\n",
       "      <th>sex</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>train_test_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_4695565</td>\n",
       "      <td>75.0</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>akiec</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_2485486</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_4407081</td>\n",
       "      <td>65.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_5173845</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_6803155</td>\n",
       "      <td>70.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>male</td>\n",
       "      <td>IL_8876724</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_6037786</td>\n",
       "      <td>55.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_1728878</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_4263601</td>\n",
       "      <td>75.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>akiec</td>\n",
       "      <td>male</td>\n",
       "      <td>IL_6263448</td>\n",
       "      <td>no</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id  age_approx anatom_site_general     dx     sex   lesion_id  \\\n",
       "0  ISIC_4695565        75.0     posterior torso  akiec  female  IL_2485486   \n",
       "1  ISIC_4407081        65.0           head/neck  akiec  female  IL_5173845   \n",
       "2  ISIC_6803155        70.0           head/neck  akiec    male  IL_8876724   \n",
       "3  ISIC_6037786        55.0           head/neck  akiec  female  IL_1728878   \n",
       "4  ISIC_4263601        75.0           head/neck  akiec    male  IL_6263448   \n",
       "\n",
       "  is_duplicate train_test_split  \n",
       "0           no            train  \n",
       "1           no            train  \n",
       "2           no            train  \n",
       "3           no            train  \n",
       "4           no            train  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identify_trainOrtest(x):\n",
    "    test_data = set(test_df['image_id'])\n",
    "    if str(x) in test_data:\n",
    "        return 'test'\n",
    "    else:\n",
    "        return 'train'\n",
    "\n",
    "#creating train_df\n",
    "data_pd['train_test_split'] = data_pd['image_id'].apply(identify_trainOrtest)\n",
    "train_df = data_pd[data_pd['train_test_split'] == 'train']\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "FPySEG1m58pu",
    "outputId": "18fd6e44-8d62-4a88-efa7-d29aa64637ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>anatom_site_general</th>\n",
       "      <th>dx</th>\n",
       "      <th>sex</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8062</th>\n",
       "      <td>ISIC_0394140</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nv</td>\n",
       "      <td>male</td>\n",
       "      <td>IL_9504564</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7405</th>\n",
       "      <td>ISIC_1413366</td>\n",
       "      <td>30.0</td>\n",
       "      <td>posterior torso</td>\n",
       "      <td>nv</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_0829927</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600</th>\n",
       "      <td>ISIC_0072951</td>\n",
       "      <td>40.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>bcc</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_3918643</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>ISIC_0067089</td>\n",
       "      <td>70.0</td>\n",
       "      <td>head/neck</td>\n",
       "      <td>bcc</td>\n",
       "      <td>male</td>\n",
       "      <td>IL_0909462</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4865</th>\n",
       "      <td>ISIC_3301933</td>\n",
       "      <td>55.0</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>mel</td>\n",
       "      <td>female</td>\n",
       "      <td>IL_6507363</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id  age_approx anatom_site_general   dx     sex   lesion_id  \\\n",
       "8062  ISIC_0394140        25.0                 NaN   nv    male  IL_9504564   \n",
       "7405  ISIC_1413366        30.0     posterior torso   nv  female  IL_0829927   \n",
       "1600  ISIC_0072951        40.0           head/neck  bcc  female  IL_3918643   \n",
       "2436  ISIC_0067089        70.0           head/neck  bcc    male  IL_0909462   \n",
       "4865  ISIC_3301933        55.0     lower extremity  mel  female  IL_6507363   \n",
       "\n",
       "     is_duplicate  \n",
       "8062           no  \n",
       "7405           no  \n",
       "1600           no  \n",
       "2436           no  \n",
       "4865           no  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Ja7jQJQb39wi"
   },
   "outputs": [],
   "source": [
    "# Image id of train and test images\n",
    "train_list = list(train_df['image_id'])\n",
    "test_list = list(test_df['image_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lBJgBAjP13q5",
    "outputId": "463d1b3f-d77c-49b0-d89e-9ce5d21a47e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "680"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEChk1DK-H8Z",
    "outputId": "612e1f17-9db3-4dea-c99e-0b7aa5ee566a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8028"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "PIoMqylGAYYZ"
   },
   "outputs": [],
   "source": [
    "# Set the image_id as the index in data_pd\n",
    "data_pd.set_index('image_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "qlR6SjeEzXsm"
   },
   "outputs": [],
   "source": [
    "train_dir_4_hr = os.path.join('dataset_ds108', 'train_dir_4_hr')\n",
    "test_dir_4_hr = os.path.join('dataset_ds108', 'test_dir_4_hr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Ja_PtDYyDPMM"
   },
   "outputs": [],
   "source": [
    "os.mkdir(train_dir_4_hr)\n",
    "os.mkdir(test_dir_4_hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PsoqCvNsgmHP"
   },
   "outputs": [],
   "source": [
    "targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "9KYMTQugCmRR"
   },
   "outputs": [],
   "source": [
    "for i in targetnames:\n",
    "  directory1=train_dir_4_hr+'/'+i\n",
    "  directory2=test_dir_4_hr+'/'+i\n",
    "  os.mkdir(directory1)\n",
    "  os.mkdir(directory2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "GL9vFa3X-ty1"
   },
   "outputs": [],
   "source": [
    "for image in train_list:\n",
    "    file_name = image+'.jpg'\n",
    "    label = data_pd.loc[image, 'dx']\n",
    "\n",
    "    # path of source image \n",
    "    source = os.path.join('dataset_ds108\\my_dir_4000\\dataset_hair_removed_', file_name)\n",
    "\n",
    "    # copying the image from the source to target file\n",
    "    target = os.path.join(train_dir_4_hr, label, file_name)\n",
    "\n",
    "    shutil.copyfile(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "hwbKrEzJ_if2"
   },
   "outputs": [],
   "source": [
    "for image in test_list:\n",
    "\n",
    "    file_name = image+'.jpg'\n",
    "    label = data_pd.loc[image, 'dx']\n",
    "\n",
    "    # path of source image \n",
    "    source = os.path.join('dataset_ds108\\my_dir_4000\\dataset_hair_removed_', file_name)\n",
    "\n",
    "    # copying the image from the source to target file\n",
    "    target = os.path.join(test_dir_4_hr, label, file_name)\n",
    "\n",
    "    shutil.copyfile(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4W8hmE2OHjQa",
    "outputId": "e57e21fb-559b-40f8-8755-d8e45bd3e8a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1281 images belonging to 1 classes.\n",
      "Found 1444 images belonging to 1 classes.\n",
      "Found 995 images belonging to 1 classes.\n",
      "Found 281 images belonging to 1 classes.\n",
      "Found 1054 images belonging to 1 classes.\n",
      "Found 2691 images belonging to 1 classes.\n",
      "Found 282 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "targetnames = ['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']\n",
    "\n",
    "# Augmenting images and storing them in temporary directories \n",
    "for img_class in targetnames:\n",
    "\n",
    "    #creating temporary directories\n",
    "    # creating a base directory\n",
    "    my_aug_dir_4_hr = 'my_aug_dir_4_hr'\n",
    "    os.mkdir(my_aug_dir_4_hr)\n",
    "    # creating a subdirectory inside the base directory for images of the same class\n",
    "    img_dir_4_hr = os.path.join(my_aug_dir_4_hr, 'img_dir_4_hr')\n",
    "    os.mkdir(img_dir_4_hr)\n",
    "\n",
    "    img_list = os.listdir('dataset_ds108/train_dir_4_hr/' + img_class)\n",
    "\n",
    "    # Copy images from the class train dir to the img_dir \n",
    "    for file_name in img_list:\n",
    "\n",
    "        # path of source image in training directory\n",
    "        source = os.path.join('dataset_ds108/train_dir_4_hr/' + img_class, file_name)\n",
    "\n",
    "        # creating a target directory to send images \n",
    "        target = os.path.join(img_dir_4_hr, file_name)\n",
    "\n",
    "        # copying the image from the source to target file\n",
    "        shutil.copyfile(source, target)\n",
    "\n",
    "    # Temporary augumented dataset directory.\n",
    "    source_path = my_aug_dir_4_hr\n",
    "\n",
    "    # Augmented images will be saved to training directory\n",
    "    save_path = 'dataset_ds108/train_dir_4_hr/' + img_class\n",
    "\n",
    "    # Creating Image Data Generator to augment images\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "\n",
    "        rotation_range=180,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "    batch_size = 50\n",
    "    aug_datagen = datagen.flow_from_directory(source_path,save_to_dir=save_path,save_format='jpg',target_size=(224, 224),batch_size=batch_size)\n",
    "\n",
    "    # Generate the augmented images\n",
    "    aug_images = 4000 \n",
    "\n",
    "    num_files = len(os.listdir(img_dir_4_hr))\n",
    "    num_batches = int(np.ceil((aug_images - num_files) / batch_size))\n",
    "\n",
    "    # creating 8000 augmented images per class\n",
    "    for i in range(0, num_batches):\n",
    "        images, labels = next(aug_datagen)\n",
    "\n",
    "    # delete temporary directory \n",
    "    shutil.rmtree('my_aug_dir_4_hr')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: img2vec_pytorch in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: torch in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from img2vec_pytorch) (2.2.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from img2vec_pytorch) (0.15.2a0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from img2vec_pytorch) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from torch->img2vec_pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from torch->img2vec_pytorch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from torch->img2vec_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from torch->img2vec_pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from torch->img2vec_pytorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from torch->img2vec_pytorch) (2024.5.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from torchvision->img2vec_pytorch) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from torchvision->img2vec_pytorch) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch->img2vec_pytorch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from requests->torchvision->img2vec_pytorch) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from requests->torchvision->img2vec_pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from requests->torchvision->img2vec_pytorch) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from requests->torchvision->img2vec_pytorch) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from sympy->torch->img2vec_pytorch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install img2vec_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\preprocess\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from img2vec_pytorch import Img2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\anaconda3\\envs\\preprocess\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\preprocess\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "img2vec = Img2Vec()\n",
    "\n",
    "data_dir = fr\"D:\\DS_2nd_year\\semester_2nd\\[DS108]\\Doan\\dataset_ds108\"\n",
    "train_dir = os.path.join(data_dir, 'train_dir_4_hr')\n",
    "test_dir = os.path.join(data_dir, 'test_dir_4_hr')\n",
    "\n",
    "data = {}\n",
    "for j, dir_ in enumerate([train_dir, test_dir]):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for category in os.listdir(dir_):\n",
    "        for img_path in os.listdir(os.path.join(dir_, category)):\n",
    "            img_path_ = os.path.join(dir_, category, img_path)\n",
    "            img = Image.open(img_path_).convert(\"RGB\")\n",
    "            img = img.resize((224, 224))\n",
    "\n",
    "            img_features = img2vec.get_vec(img)\n",
    "\n",
    "            features.append(img_features)\n",
    "            labels.append(category)\n",
    "\n",
    "    data[['training_data', 'testing_data'][j]] = features\n",
    "    data[['training_labels', 'testing_label'][j]] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['training_data', 'training_labels', 'testing_data', 'testing_label'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data['training_data']) == len(data['training_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('data_6_hr.npz', **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load('data_4_hr.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(NpzFile 'data_4_hr.npz' with keys: training_data, training_labels, testing_data, testing_label)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "model.fit(test_data['training_data'], test_data['training_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       0.41      0.70      0.52        40\n",
      "         bcc       0.68      0.64      0.66        98\n",
      "         bkl       0.43      0.24      0.31        50\n",
      "          df       0.50      0.26      0.34        19\n",
      "         mel       0.64      0.44      0.52       146\n",
      "          nv       0.76      0.89      0.82       309\n",
      "        vasc       0.59      0.56      0.57        18\n",
      "\n",
      "    accuracy                           0.67       680\n",
      "   macro avg       0.57      0.53      0.53       680\n",
      "weighted avg       0.66      0.67      0.66       680\n",
      "\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 28   7   0   0   1   4   0]\n",
      " [ 13  63   7   2   3   6   4]\n",
      " [ 15   4  12   0   7  12   0]\n",
      " [  1   3   1   5   2   7   0]\n",
      " [  8   7   7   2  64  56   2]\n",
      " [  3   4   1   1  23 276   1]\n",
      " [  0   5   0   0   0   3  10]]\n",
      "\n",
      "Test Accuracy: 0.6735294117647059\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_data['testing_label'], model.predict(test_data['testing_data'])))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(test_data['testing_label'], model.predict(test_data['testing_data'])))\n",
    "\n",
    "# Calculate and print additional metrics\n",
    "test_accuracy = accuracy_score(test_data['testing_label'], model.predict(test_data['testing_data']))\n",
    "print(f\"\\nTest Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(test_data['training_data'], test_data['training_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       0.28      0.53      0.37        40\n",
      "         bcc       0.62      0.50      0.55        98\n",
      "         bkl       0.24      0.20      0.22        50\n",
      "          df       0.50      0.11      0.17        19\n",
      "         mel       0.58      0.34      0.43       146\n",
      "          nv       0.71      0.89      0.79       309\n",
      "        vasc       0.73      0.44      0.55        18\n",
      "\n",
      "    accuracy                           0.61       680\n",
      "   macro avg       0.52      0.43      0.44       680\n",
      "weighted avg       0.61      0.61      0.59       680\n",
      "\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 21   7   2   0   5   5   0]\n",
      " [ 28  49   9   0   2  10   0]\n",
      " [ 10  10  10   0   7  13   0]\n",
      " [  3   3   0   2   2   9   0]\n",
      " [  5   4  15   0  50  69   3]\n",
      " [  7   2   5   1  19 275   0]\n",
      " [  0   4   0   1   1   4   8]]\n",
      "\n",
      "Test Accuracy: 0.6102941176470589\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_data['testing_label'], model.predict(test_data['testing_data'])))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(test_data['testing_label'], model.predict(test_data['testing_data'])))\n",
    "\n",
    "# Calculate and print additional metrics\n",
    "test_accuracy = accuracy_score(test_data['testing_label'], model.predict(test_data['testing_data']))\n",
    "print(f\"\\nTest Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light Gradient-Boost Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\roaming\\python\\python311\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\envs\\preprocess\\lib\\site-packages (from lightgbm) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076635 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130560\n",
      "[LightGBM] [Info] Number of data points in the train set: 27633, number of used features: 512\n",
      "[LightGBM] [Info] Start training from score -1.934468\n",
      "[LightGBM] [Info] Start training from score -1.923261\n",
      "[LightGBM] [Info] Start training from score -1.925244\n",
      "[LightGBM] [Info] Start training from score -1.983220\n",
      "[LightGBM] [Info] Start training from score -1.954962\n",
      "[LightGBM] [Info] Start training from score -1.922519\n",
      "[LightGBM] [Info] Start training from score -1.979808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "model = LGBMClassifier(boosting_type='gbdt', num_leaves=31, learning_rate=0.1, n_estimators=100)\n",
    "model.fit(test_data['training_data'], test_data['training_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       akiec       0.34      0.60      0.43        40\n",
      "         bcc       0.69      0.60      0.64        98\n",
      "         bkl       0.35      0.24      0.29        50\n",
      "          df       0.33      0.16      0.21        19\n",
      "         mel       0.60      0.48      0.53       146\n",
      "          nv       0.77      0.86      0.81       309\n",
      "        vasc       0.47      0.44      0.46        18\n",
      "\n",
      "    accuracy                           0.65       680\n",
      "   macro avg       0.51      0.48      0.48       680\n",
      "weighted avg       0.64      0.65      0.64       680\n",
      "\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 24   6   2   0   4   4   0]\n",
      " [ 15  59   8   2   7   3   4]\n",
      " [ 14   5  12   1   6  12   0]\n",
      " [  2   3   0   3   4   7   0]\n",
      " [  8   7   8   0  70  50   3]\n",
      " [  7   3   3   2  26 266   2]\n",
      " [  1   3   1   1   0   4   8]]\n",
      "\n",
      "Test Accuracy: 0.65\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(test_data['testing_label'], model.predict(test_data['testing_data'])))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(test_data['testing_label'], model.predict(test_data['testing_data'])))\n",
    "\n",
    "# Calculate and print additional metrics\n",
    "test_accuracy = accuracy_score(test_data['testing_label'], model.predict(test_data['testing_data']))\n",
    "print(f\"\\nTest Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "ResNet50.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
